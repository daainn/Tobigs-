{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IabIA345uW2s"
   },
   "source": [
    "## 투빅스 18기 Week1 Framework 과제 - 18기 이다인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt1H5o-poBfF"
   },
   "source": [
    "## 1. \n",
    "#### 패키지와 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B1X8mpttn-Vi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transfroms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWzCD1LRn-vT"
   },
   "source": [
    "## 2.\n",
    "#### 하드웨어 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7YfkyOQioSBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu is available\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "#위 코드가 필요한 이유는 학습을 위한 실험에서 무작위성을 컨트롤 하기 위함\n",
    "print(device + \" is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ_MTPYyoUbo"
   },
   "source": [
    "## 3.\n",
    "#### 학습에 사용될 파라미터 설정하기\n",
    "\n",
    "learning_rate = 학습률   \n",
    "epoch = 전체 트레이닝 셋이 신경망을 통과한 횟수  \n",
    "num_classes = 출력 데이터의 크기  \n",
    "batch_size = 전체 트레이닝 셋을 여러 작은 그룹으로 나누었을 때 하나의 소그룹에 속하는 데이터의 수  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LQee8cNioUjL"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDVGW05NoUri"
   },
   "source": [
    "## 4.\n",
    "#### MNIST 데이터셋을 로드하여 train,test 데이터셋을 배치형태로 변환시켜주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hn6FU1proUxO"
   },
   "outputs": [],
   "source": [
    "#MNIST의 데이터셋을 다운로드 하였으며, ToTensor()를 이용하여 데이터를 0~255 값에서 0~1사이의 값으로 변환하였다.\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() \n",
    "    ])\n",
    ")\n",
    "\n",
    "#MNIST의 데이터셋을 다운로드 하였으며, ToTensor()를 이용하여 데이터를 0~255 값에서 0~1사이의 값으로 변환하였다.\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "#train_loader와 test_loader를 통해 MNIST에서 불러온 데이터를 batch형태로 변환시켜주기\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35KJ9PP9oVCI"
   },
   "source": [
    "## 5.\n",
    "#### input size 파악하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_aJMrF54oVGV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(train_set)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bxJSYFToVLy"
   },
   "source": [
    "## 6. \n",
    "#### 모델 구축 및 손실 함수와 옵티마이저 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7yzDsZ9roVQM"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): #컨볼루션 신경망 정의\n",
    "  def __init__(self): \n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        #input size = 28x28에서 컨볼루션 신경망 출력사이즈 공식을 사용한 후 mxpooling하여 12x12로 변환\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # input channel = 10, filter = 20, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        #12x12에서 컨볼루션 신경망 출력사이즈 공식을 사용한 후 maxpooling하여 4x4로 변환\n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False)\n",
    "        self.mp = nn.MaxPool2d(2) \n",
    "        # 오버피팅과 하드웨어 리소스 낭비를 방지하기 위해 맥스풀링\n",
    "        self.fc1 = nn.Linear(320,100) \n",
    "        # 100개의 라벨을 출력하도록 하는 fully connected layer\n",
    "        self.fc2 = nn.Linear(100,10) \n",
    "        # 100개를 다시 10개의 라벨으로 출력하도록 하는 fully connected layer\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = F.relu(self.mp(self.conv1(x))) # 위에서 정의된 convolution layer 1에 relu를 씌움\n",
    "        x = F.relu(self.mp(self.conv2(x))) # 위에서 정의된 convolution layer 2에 relu를 씌움\n",
    "        #relu 함수는 기울기 소실 문제 방지를 위하여 사용하는 활성화 함수\n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(x.size(0), -1) # 전결합층을 위하여 flatter시키는 작업\n",
    "        x = self.fc1(x) #위에서 정의된 첫번째 fully connected layer에 삽입\n",
    "        x = self.fc2(x) #위에서 정의된 두번째 fully connected layer에 삽입\n",
    "        return F.log_softmax(x) #logsoftmax 적용 : 입력값을 0에서 1사이의 값으로 정규화시키는 softmax함수에 log를 취한 함수로 더 빠르고 안정적이다\n",
    "    \n",
    " \n",
    "model = ConvNet().to(device) #GPU에서 연산 수행\n",
    "#손실함수(=Cost Function)와 optimizer 선택\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "#### epoch 수 만큼 모델 학습 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_12992\\3650849640.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.31730783\n",
      "[Epoch:    2] cost = 0.114742726\n",
      "[Epoch:    3] cost = 0.0868237242\n",
      "[Epoch:    4] cost = 0.0750293583\n",
      "[Epoch:    5] cost = 0.0662271008\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): \n",
    "    avg_cost = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        #모든 model의 gradient 값을 0으로 초기화, 만일 초기화 하지 않으면 미분값이 계속 누적되기 때문\n",
    "        hypothesis = model(data) #forward연산 수행\n",
    "        cost = criterion(hypothesis, target) # 비용함수로 output과 target의 loss 계산\n",
    "        cost.backward() \n",
    "        optimizer.step() #모델의 학습 파라미터 갱신\n",
    "        avg_cost += cost / len(train_loader) # loss 값을 변수에 누적한 후 trainloader의 개수로 나눔\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "#### 모델 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_12992\\3650849640.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  98.57 %\n"
     ]
    }
   ],
   "source": [
    "model.eval() # evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 차단시키는 함수\n",
    "with torch.no_grad(): #grad 해제\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        out = model(data)\n",
    "        preds = torch.max(out.data, 1)[1] #가장 높은 값 출력\n",
    "        total += len(target) \n",
    "        correct += (preds==target).sum().item() # 예측값과 실제값 비교\n",
    "        \n",
    "    print('Test Accuracy: ', 100.*correct/total, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Framework_PyTorch.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
